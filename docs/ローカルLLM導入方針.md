# ローカルLLM導入方針

最終更新: 2026-02-14

## 結論

- **導入は可能**です。
- ただし IME に実運用レベルで組み込むには、以下3点を確定する必要があります。
  1. 推論ランタイム（`llama.cpp` / `MLC LLM` / 端末内AICore）
  2. モデル配布方式（初回DL / 手動配置）
  3. モデル容量と遅延（IMEで許容できるか）

## 現在の実装状態

- 設定画面でローカルLLMモデルの配置有無を検知表示
- 変換処理は端末内ローカル処理のみ（外部API連携なし）
- 検知対象:
  - `files/local_llm/model.gguf`
  - `externalFiles/local_llm/model.gguf`
- まだ推論エンジン本体（llama.cpp / MLC LLM）は未組み込み

## 現実的な方式

1. `llama.cpp (GGUF)` 組み込み  
- 長所: 汎用性が高い  
- 短所: NDK/JNI実装とAPK肥大化、端末負荷が大きい

2. `MLC LLM` 採用  
- 長所: Android向け実績がある  
- 短所: ランタイム統合コストが高い

3. 端末内AIランタイム（対応機種のみ）  
- 長所: 高速・省電力の可能性  
- 短所: 端末依存が大きい（全ユーザーで動かない）

## IMEに入れる場合の最小要件

- 変換処理タイムアウト（例: 700ms）
- タイムアウト時のローカルルール即時フォールバック
- メモリ上限超過時の自動停止
- API通信なし（完全オフライン）

## 次段階の実装案

1. まずは「ローカルLLM専用モード」を設定に追加（既存変換と分離）
2. 1モデルだけ固定してPoC（短文整形のみ）
3. Fold 7実機で遅延・発熱・電池消費を計測し、採用可否を判断
